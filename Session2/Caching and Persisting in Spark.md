# cache() vs persist() in Apache Spark

## ðŸš€ Purpose
Both `cache()` and `persist()` are used to **store RDDs or DataFrames** in memory to **avoid recomputation**. This is crucial for iterative algorithms and performance optimization.

---

## ðŸ§  Basic Difference

| Feature        | `cache()`                            | `persist()`                        |
|----------------|----------------------------------------|-------------------------------------|
| Behavior       | Stores in memory, fallback to disk    | Customizable storage levels         |
| Default Mode   | MEMORY_AND_DISK                       | Needs explicit level if not default |
| Flexibility    | âŒ Fixed storage level                 | âœ… Flexible                         |
| Serialization  | âŒ No (unless using *_SER levels)     | âœ… Optional with *_SER levels       |

---

## ðŸ§¾ What is `cache()`?

```scala
df.cache()
```

- Shorthand for:
  ```scala
  df.persist(StorageLevel.MEMORY_AND_DISK)
  ```
- Stores as much as possible in **memory**.
- If not enough memory, **spills to disk**.
- Automatically **reuses** the stored result in subsequent actions.

---

## ðŸ§¾ What is `persist()`?

```scala
import org.apache.spark.storage.StorageLevel

df.persist(StorageLevel.MEMORY_ONLY)
```

- More **flexible** and allows different storage options.
- You can store:
  - Only in memory
  - Only on disk
  - Serialized or un-serialized
  - With or without replication

---

## ðŸ” When to Use What

| Use Case                             | Method       | Why                                        |
|--------------------------------------|--------------|---------------------------------------------|
| Default memory + disk fallback       | `cache()`    | Simple, quick, covers common case           |
| Memory-only strategy                 | `persist()`  | If recomputation is cheap                   |
| Memory is limited, avoid failures    | `persist()`  | Use DISK_ONLY or MEMORY_AND_DISK_SER        |
| Need serialization to save space     | `persist()`  | Use *_SER to reduce memory footprint        |

---

## ðŸ“¦ Popular Storage Levels

| Storage Level                    | Description                                                       |
|----------------------------------|-------------------------------------------------------------------|
| MEMORY_ONLY                      | Store only in memory, recompute if not enough                     |
| MEMORY_AND_DISK                  | Try memory, spill to disk if needed (default for `cache()`)       |
| MEMORY_ONLY_SER                 | Serialized in memory, saves space but more CPU                    |
| MEMORY_AND_DISK_SER             | Serialized + spill to disk                                        |
| DISK_ONLY                        | Store only on disk                                                |
| OFF_HEAP (if enabled)            | Store in off-heap memory (requires Spark config change)           |

---

## ðŸ’½ What Does "Disk" Mean in Spark?

When Spark says it stores data on **disk**, it refers to:

> ðŸ—‚ï¸ The **local file system of the executor node**, not HDFS or S3.

### ðŸ”§ Storage Path
- Spark writes to temporary directories like `/tmp` or paths under:
  ```
  spark.local.dir (e.g., /tmp or /data/spark-tmp)
  ```
- You can change this with:
  ```bash
  --conf spark.local.dir=/your/custom/path
  ```

### âš ï¸ Important Notes:
- âŒ Not HDFS, S3, or any external cloud storage
- âœ… Temporary local disk used during job execution
- ðŸ“¦ Useful when memory is insufficient but recomputation is costly

---

## ðŸ§  What Does "Memory" Mean in Spark?

When Spark refers to "memory", it means:

> ðŸš€ The **RAM of the executor JVM process** running on each worker node.

### ðŸ“¦ Breakdown of Executor Memory
```
Total Executor Memory
â”œâ”€â”€ Reserved Memory (fixed ~300MB)
â”œâ”€â”€ User Memory (25%)
â”œâ”€â”€ Spark Memory (75%)
     â”œâ”€â”€ Execution Memory (e.g., joins, shuffles)
     â””â”€â”€ Storage Memory (for caching/persisting)
```
Only the **Storage Memory portion** is used to store cached RDDs/DataFrames.

### ðŸ” Example
If an executor has 8 GB:
- ~300MB â†’ Reserved
- ~1.9 GB â†’ User Memory
- ~5.8 GB â†’ Spark Memory (shared between execution and storage)

> Cached data lives inside this 5.8 GB shared space under the storage region.

### ðŸ§¯ If Memory Is Not Enough?
- Spark may **evict older blocks** (LRU cache)
- If using `MEMORY_ONLY`, it recomputes when accessed again
- If using `MEMORY_AND_DISK`, it spills to disk

---

## âœ… Best Practices
- Use `cache()` when starting out or for prototyping
- Use `persist()` with specific levels for tuning performance/memory
- Monitor storage usage via **Spark UI â†’ Storage tab**
- Donâ€™t forget to `unpersist()` when done:
  ```scala
  df.unpersist()
  ```
